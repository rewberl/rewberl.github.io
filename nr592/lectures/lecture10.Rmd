---
title: |
  | &nbsp;
  | Week 5, Lecture 10
  | Advanced statistical methods, part I: Ecological analyses, ordinal data, and dimensionality reduction
author: |
  | &nbsp;
  | Richard E.W. Berl
date: |
  | Spring 2019
  | &nbsp;
output:
  html_document:
    highlight: haddock
    theme: flatly
  pdf_document:
    highlight: haddock
---

## Dimensionality reduction

Let's load our Bob Marshall onsite survey data back in:

```{r}
bm = read.csv("./data/BMWC2004_onsitedata.csv", header=T, na.strings="88",
              stringsAsFactors=F)
bm$recent_f[bm$recent_f == 0] = NA

bmLik = bm
bmLik[,36:45] = lapply(bmLik[,36:45], function(x) ordered(x))
```

And recreate our correlation matrix:

```{r, warning=FALSE}
library(lavaan)

bmLikCor = lavCor(bmLik[,36:45])
```

Let's also load our previously saved "best" Christmas Bird Count data:

```{r}
best = read.csv("./data/fcbirdbest.csv", header=T, row.names=1)
```

And finally, we'll also bring back our old Indo-European folktale data from [Lecture 03](./lecture03.html):

```{r, warning=FALSE}
library(readxl)
```

```{r}
folktales = as.data.frame(read_xlsx(path="./data/rsos150645supp1.xlsx",
                                    sheet=1, range="A2:JP52"))
colnames(folktales)[1] = "society"
```

<br>

### Multidimensional scaling

Primarily used as a way to visualize a distance matrix

```{r, eval=FALSE}
install.packages("psych")
```

```{r, warning=FALSE}
library(psych)
```

```{r, eval=FALSE}
?cor2dist
```

```{r}
bmLikDist = as.dist(cor2dist(bmLikCor))
bmLikDist
```

<br>

#### **Classical**

```{r, eval=FALSE}
?cmdscale
```

```{r}
bmCMD = cmdscale(bmLikDist)
bmCMD
```

```{r}
plot(bmCMD[,1], bmCMD[,2])
text(bmCMD[,1], bmCMD[,2] + 0.04,
     labels=rownames(bmCMD), col="blue", cex=0.7)
```

<br>

#### **Nonmetric**

Tries to reproduce ranks of distances rather than distance values themselves

```{r, warning=FALSE}
library(MASS)
```

```{r, eval=FALSE}
?isoMDS
```

```{r}
bmNMD = isoMDS(bmLikDist)
bmNMD
```

```{r}
plot(bmNMD$points[,1], bmNMD$points[,2])
text(bmNMD$points[,1], bmNMD$points[,2] + 0.04,
     labels=rownames(bmNMD$points), col="blue", cex=0.7)
```

<br>

```{r, warning=FALSE}
library(vegan)
```

```{r, eval=FALSE}
?metaMDS
```

```{r}
head(best)
```

```{r}
bestNMD = metaMDS(best)
bestNMD
```

```{r}
plot(bestNMD)
text(bestNMD$points[,1], bestNMD$points[,2] + 0.025,
     labels=rownames(bestNMD$points), col="blue", cex=0.7)
text(bestNMD$species[,1], bestNMD$species[,2] + 0.025,
     labels=rownames(bestNMD$species), col="red", cex=0.7)
```

<br><br>

### Principal components analysis

```{r, eval=FALSE}
?princomp
```

```{r, eval=FALSE}
?psych::principal
```

```{r, eval=FALSE}
?psych::tetrachoric
```

```{r}
folk = as.data.frame(t(folktales[,-1]))
colnames(folk) = folktales$society
folk[1:5,1:10]
```

```{r}
str(folk)
```

**Tetrachoric correlation**

Binary data

```{r}
folkCor = tetrachoric(folk)$rho
folkCor[1:10,1:10]
```

Note warnings about bivariate cell corrections. Okay to leave as 0.5. Source:

Savalei, V. (2011). What to do about zero frequency cells when estimating polychoric correlations. Structural Equation Modeling, 18(2), 253-273. doi: [10.1080/10705511.2011.557339](https://doi.org/10.1080/10705511.2011.557339)

Smoothing ensures the matrix isn't singular and is usually also okay.

```{r}
folkPCA = principal(folkCor, 2, rotate="none")
folkPCA
```

```{r}
plot(folkPCA$loadings[,1], folkPCA$loadings[,2])
text(folkPCA$loadings[,1], folkPCA$loadings[,2] + 0.04,
     labels=rownames(folkPCA$loadings), col="blue", cex=0.7)
```


<br><br>

### Cluster analysis

#### **Hierarchical clustering**

```{r, eval=FALSE}
?hclust
```

```{r}
bmHC = hclust(bmLikDist, method="ward.D2")
```

```{r}
plot(bmHC)
```

```{r}
heatmap(as.matrix(bmLikDist),
        hclustfun=function(x) hclust(x, method="ward.D2"))
```

```{r, eval=FALSE}
install.packages("pvclust")
```

```{r, warning=FALSE}
library(pvclust)
```

```{r, eval=FALSE}
?pvclust
```

Needs raw numeric data; does not allow distance matrix as input

```{r}
# Note: Takes some time to run
bmPVHC = pvclust(bm[,36:45], method.hclust="ward.D2")
```

```{r}
plot(bmPVHC)
pvrect(bmPVHC)
```

Red = "AU" (Approximately Unbiased) _p_value: 1 - _p_-value (>95 is "significant")

Green = "BP" (Bootstrap Probability): percent of times the tree-building algorithm produced that branch

<br>

#### **K-means clustering**

```{r, eval=FALSE}
?kmeans
```

Does not work with missing values

"Elbow method"

```{r, eval=FALSE}
install.packages("factoextra")
```

```{r, warning=FALSE}
library(factoextra)
```

```{r}
fviz_nbclust(best, kmeans, "wss")
```

```{r}
fviz_nbclust(best, kmeans, "silhouette")
```

```{r}
bestKM4 = kmeans(best, 4)
bestKM4
```

```{r}
fviz_cluster(bestKM4, data=best, show.clust.cent=F)
```

```{r}
bestKM2 = kmeans(best, 2)
fviz_cluster(bestKM2, data=best, show.clust.cent=F)
```

This clustered by row (year). What if we want to cluster by column (species) instead?

```{r}
fviz_nbclust(t(best), kmeans, "wss", k.max=nrow(t(best)) - 1)
```

```{r}
fviz_nbclust(t(best), kmeans, "silhouette", k.max=nrow(t(best)) - 1)
```

```{r}
bestTKM2 = kmeans(t(best), 2)
bestTKM2
```

```{r}
fviz_cluster(bestTKM2, data=t(best), show.clust.cent=F)
```

<br>

#### **K-medoids clustering**

```{r, eval=FALSE}
install.packages("cluster")
```

```{r, warning=FALSE}
library(cluster)
```

```{r, eval=FALSE}
?pam
```

```{r, eval=FALSE}
install.packages("fpc")
```

```{r, warning=FALSE}
library(fpc)
```

```{r, eval=FALSE}
?pamk
```

```{r}
folkDist = as.dist(cor2dist(folkCor))
as.matrix(folkDist)[1:10,1:10]
```

```{r}
dim(folkDist)
nrow(folkDist)
ncol(folkDist)
class(folkDist)

attributes(folkDist)
```

```{r}
folkPAMK = pamk(folkDist, diss=T, krange=2:attributes(folkDist)$Size-1)
folkPAMK
```

```{r}
folkPAM = pam(folkDist, diss=T, k=folkPAMK$nc)
folkPAM
```

```{r}
plot(folkPAM)
```

```{r, eval=FALSE}
?clusplot
```

```{r}
clusplot(folkPAM, lines=0, color=T, labels=3)
```

<br><br>

### Exploratory factor analysis

```{r, eval=FALSE}
?factanal
```

```{r, eval=FALSE}
?psych::fa
```

```{r, eval=FALSE}
install.packages("GPArotation")
```

```{r, eval=FALSE}
?psych::vss
?psych::fa.parallel
```

```{r}
vss(bmLikCor, n=nrow(bmLikCor)-1, rotate="oblimin", fm="ml", n.obs=nrow(bmLik))
```

```{r}
fa.parallel(bmLikCor, fm="ml", n.obs=nrow(bmLik))
```

```{r}
bmEFA = fa(bmLikCor, nfactors=4, rotate="oblimin", fm="ml", n.obs=nrow(bmLik))
bmEFA
```

```{r}
bmEFA$loadings
```


<br><br>


([pdf](./lecture10.pdf) / [Rmd](./lecture10.Rmd))

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
